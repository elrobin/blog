{
  "hash": "b6092cabef6b9dd2b8b0ca9da7f1d473",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Downloading data from Scopus API\"\nauthor: \"N. Robinson-Garcia\"\ndate: \"2024-11-27\"\ntoc: true\ncategories: [coding]\n---\n\n\n\nThis is a small tutorial on how to work with the Scopus API using R. Most bibliographic databases allow nowadays downloading bibliographic data through their API in an automated way if you are registered and have an institutional subscription. In many cases, they lack of a good documentation, this is not the case for Elsevier, who have quite a lot of info on how to work with their data.\n\n## Step 1. Setup API Access\n\nFirst thing you need to do is to go to the [Elsevier Developers Portal](https://dev.elsevier.com/) and request an API Key.\n\n![](images/elsevierdvs.png)\n\nWhen you request a key, it asks you to access through your institutional account and will register an API key under your profile.\n\n## Step 2. Environment setup\n\nNow it is time to make sure we have all the packages needed to download and process data. These are:\n\n-   `httr`. Provides functions for working with HTTP requests and responses, making it easy to interact with web APIs\n\n-   `jsonlite`. A package to parse, process and generate JSON data.\n\n-   `tidyverse`. A collection of R packages for data manipulation, visualization and analysis. Helps to organize and work with data retrieved from the API.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load libraries\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks jsonlite::flatten()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n:::\n\n\n\nNext step is to store the API key in an `.Renviron` file. For this we first need to open the `.Renviron` file in RStudio:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile.edit(\"~/.Renviron\")\n```\n:::\n\n\n\nAdd a new line to the file with this info:\n\n\n\n::: {.cell}\n\n```{.makefile .cell-code}\nSCOPUS_API_KEY=your_api_key_here\n```\n:::\n\n\n\nNow let's try it works:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Retrieve API from .Renviron\napi_key <- Sys.getenv(\"SCOPUS_API_KEY\")\n\n# Test API call to check validity of the key\nresponse <- GET(\"https://api.elsevier.com/content/search/scopus\",\n                add_headers(\"X-ELS-APIKey\" = api_key),\n                query = list(query = \"AUTHOR-NAME(Robinson-Garcia)\", count = 1))\n\n# Check status\nif (status_code(response) == 200) {\n  print(\"API key is valid and working.\")\n} else {\n  print(paste(\"Error:\", status_code(response), \"Check your API key or access permissions.\"))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"API key is valid and working.\"\n```\n\n\n:::\n:::\n\n\n\n## Step 3. Reading and preparing the list of Author IDs\n\nIn my case I already have a list of publications with their author IDs per row. I want to work only with the Author IDs and clean it so that I have one by row in a vector for querying later on the API.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)     # For reading the CSV file\n\n# Step 1: Import the CSV file\n# Replace \"your_file.csv\" with your actual file path\ndata <- read_csv(\"G:/Mi unidad/1. Work sync/Projects/z2025_01-SELECT/Contributions-inequalites/raw_data/contrib_data.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 675080 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (7): doi, auid_list, affiliation_id_list, author_affiliation_mapping, s...\ndbl (22): Eid, Year, n_authors, source_id, CitationCount, DDR, DDA, SV_topic...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Step 2: Extract and clean the 'auid_list' column\nauthor_ids <- data %>%\n  select(auid_list) %>%               # Select the relevant column\n  separate_rows(auid_list, sep = \",\") %>% # Split each row by commas\n  mutate(auid_list = str_trim(auid_list)) %>% # Trim whitespace\n  distinct(auid_list) %>%                 # Remove duplicate IDs\n  pull(auid_list)                         # Extract as a vector\n\n# Optional: Check the length of the vector\nlength(author_ids)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2082499\n```\n\n\n:::\n:::\n\n\n\nI end up with over 2M authors.\n\n## Step 4. Query the API for metadata\n\nLet's create a function to download the data we want:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to query Scopus API for author metadata\nquery_author <- function(author_id, api_key, output_dir = \"author_data\") {\n  # Ensure the output directory exists\n  if (!dir.exists(output_dir)) dir.create(output_dir)\n  \n  # Construct the API URL\n  url <- paste0(\"https://api.elsevier.com/content/author/author_id/\", author_id)\n  \n  # Query the API\n  response <- GET(url,\n                  add_headers(\"X-ELS-APIKey\" = api_key),\n                  query = list(httpAccept = \"application/json\"))\n  \n  if (status_code(response) == 200) {\n    # Parse the response content\n    content_raw <- content(response, as = \"text\", encoding = \"UTF-8\")\n    content <- fromJSON(content_raw)\n    \n    # Save to a JSON file\n    output_file <- file.path(output_dir, paste0(author_id, \".json\"))\n    write_json(content, output_file, pretty = TRUE)\n    return(TRUE)  # Indicate success\n  } else {\n    # Log the error\n    print(paste(\"Error: Status code\", status_code(response), \"for author ID:\", author_id))\n    return(FALSE)  # Indicate failure\n  }\n}\n```\n:::\n\n\n\nAnd now a test to see if everything works:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nau_data <- query_author(\"36712349900\", api_key)\nprint(au_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n\n## Step 4. Create loop and download\n\n#### **1. Create the Loop and Batch Logic**\n\n-   Implement a loop to process **author IDs** in batches.\n\n-   Write a function to distribute **batches across API keys** for parallel processing.\n\n**Steps**:\n\n1.  Split the full list of author IDs into manageable batches.\n\n2.  Assign batches to available API keys, ensuring even distribution.\n\n3.  Process each batch sequentially or in parallel using the respective API key.\n\n4.  Save the results incrementally as JSON files.\n\n#### **2. Test the Code with a Small Dataset**\n\n-   Use a sample dataset of **100 author IDs** to validate the process before scaling up.\n\n-   Use **2 API keys** for this test to confirm:\n\n    -   Batch splitting and key assignment work correctly.\n\n    -   Parallelization works as expected.\n\n    -   JSON files are saved correctly.\n\n-   Test and document the maximum batch size (e.g., **100, 500, 1000 IDs per batch**) that can run smoothly without exceeding memory or rate limits.\n\n#### **3. Execute the Parallel Download with 50 API Keys**\n\n-   After validating the test, set up the full process to use **50 API keys** for the **entire dataset**.\n\n-   Ensure:\n\n    -   API keys are evenly distributed across batches.\n\n    -   Pauses are implemented between batches if necessary to comply with API rate limits.\n\n    -   All data is saved incrementally as JSON files.\n\n#### **Additional Notes**\n\n1.  **API Key Management**:\n\n    -   Confirm that all 50 API keys are valid and have sufficient quotas (20,000 requests/week per key).\n\n    -   Monitor usage during the process to avoid exceeding limits.\n\n2.  **Error Handling**:\n\n    -   Ensure failed queries are logged (e.g., into an `errors.csv` file) for retries later.\n\n3.  **Resuming Progress**:\n\n    -   Include a mechanism to skip already processed IDs by checking for existing JSON files in the output directory.\n\n### **Next Steps Summary**\n\n1.  **Batch Testing**:\n\n    -   Test batch size limits with a small dataset of **100 IDs** using **2 API keys**.\n\n2.  **Parallel Processing**:\n\n    -   Implement parallel processing with **50 API keys** and scale up for the full dataset.\n\n3.  **Error Handling & Logs**:\n\n    -   Log any failed queries for retries later.\n\n4.  **Data Storage**:\n\n    -   Save each author’s data as a JSON file incrementally.\n\n# Save final results\n\nwrite_csv(results, \"scopus_author_data_final.csv\")\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}