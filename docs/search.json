[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Do not read",
    "section": "",
    "text": "Downloading data from Scopus API\n\n\n\n\n\n\ncoding\n\n\n\n\n\n\n\n\n\nNov 27, 2024\n\n\nN. Robinson-Garcia\n\n\n\n\n\n\n\n\n\n\n\n\nMore on multiauthorship\n\n\n\n\n\n\nreviews\n\n\n\n\n\n\n\n\n\nNov 26, 2024\n\n\nN. Robinson-Garcia\n\n\n\n\n\n\n\n\n\n\n\n\nScientific authorship and dealing with multiauthorship\n\n\n\n\n\n\nreviews\n\n\n\n\n\n\n\n\n\nNov 25, 2024\n\n\nN. Robinson-Garcia\n\n\n\n\n\n\n\n\n\n\n\n\nHello world\n\n\n\n\n\n\ngibberish\n\n\n\n\n\n\n\n\n\nNov 24, 2024\n\n\nN. Robinson-Garcia\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/26-11-2024/index.html",
    "href": "posts/26-11-2024/index.html",
    "title": "More on multiauthorship",
    "section": "",
    "text": "Image generated with DALL-E\n\n\nLately I am very interested on authorship and more specifically, on the problems this concept brings in terms of allocation of credit among researchers given the current collaborative nature of research. This is very much related to the book I am writing on diversity and recognition in academia.\nHere some notes for some of the papers I am reading.\nCronin, B. (2001). Hyperauthorship: A postmodern perversion or evidence of a structural shift in scholarly communication practices? Journal of the American Society for Information Science and Technology, 52(7), 558–569. https://doi.org/10.1002/asi.1097\nHe focuses on the issues arising in biomedical sciences with regard to honorary authorship and authorship inflation. Takes a historical perspective, from the birth of scientific authorship to the development of the current scientific publishing system. It discusses disciplinary differences on authorship order. Authorship is tied to credit and responsibility, but also is linked with an individualistic notion and not a collective endeavour which is an anachronistic conception of science.\nCollaboration becomes a reality especially after WWII, with the expansion of ‘big science’ which required funding, infrastructure and coordination and management. Currently it is the norm, with some fields and projects requiring the development of an internal structure and division of labour while others are more informal levels of collaboration. He mentions the suggestion by (Rennie, Yank, and Emanuel 1997) of abandoning the concept of author in favour of alternatives such as contributor or guarantor. As we know this proposal later gained track within the biomedical sciences with the CRediT taxonomy becoming a NISO standard more than 15 years later (Allen et al. 2014). This has to do with the inefficacy of the initial solution proposed by ICMJE which was to make all authors responsible of the complete content of a paper.\nStill, Cronin is critical with this proposal because it is still difficult to enforce responsibility in cases of what he terms hyperauthorship. He entertains the idea of using the ‘Acknowledgements’ section for contributions of second order, but recognizes also its limitations. Finally, he discusses the HEP model where all the recognition and authorship discussions are ridden internally, while an external observer would be incapable of administrating credit through authorship.\nWalsh, J. P., & Lee, Y.-N. (2015). The bureaucratization of science. Research Policy, 44(8), 1584–1600. https://doi.org/10.1016/j.respol.2015.04.010\nThis paper examines how scientific teams organize internally based on their size, interdisciplinarity and task interdependence. It uses organizational theory (similarly to (Whitley 2000)) and emphasizes the importance of internal structure to ensure a successful performance. They make some interesting points, such as the changes on the training model of scientists from the apprentice-model kind of model suggested by (Laudel and Gläser 2008) (although not cited in the paper) to an industrialized model of scientific careers with more of a professor-employer kind of model. Again they bring up the misalignment between credit allocation and multiauthorship.\n\n\n\n\nReferences\n\nAllen, Liz, Jo Scott, Amy Brand, Marjorie Hlava, and Micah Altman. 2014. “Publishing: Credit Where Credit Is Due.” Nature 508 (7496): 312–13. https://doi.org/10.1038/508312a.\n\n\nLaudel, Grit, and Jochen Gläser. 2008. “From Apprentice to Colleague: The Metamorphosis of Early Career Researchers.” Higher Education 55 (3): 387–406. https://doi.org/10.1007/s10734-007-9063-7.\n\n\nRennie, Drummond, Veronica Yank, and Linda Emanuel. 1997. “When Authorship Fails: A Proposal to Make Contributors Accountable.” JAMA 278 (7): 579–85. https://doi.org/10.1001/jama.1997.03550070071041.\n\n\nWhitley, Richard. 2000. The Intellectual and Social Organization of the Sciences. 2nd ed. Oxford: Oxford University Press."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/24-11-2024/index.html",
    "href": "posts/24-11-2024/index.html",
    "title": "Scientific authorship and dealing with multiauthorship",
    "section": "",
    "text": "Illustration from Davis Parkins, Source: Nature\n\n\nNotes from Biagioli, M. (2013). Rights or Rewards? Changing Frameworks of Scientific Authorship. In M. Biagioli & P. Gallison (eds) Scientific Authorship: Credit and Intellectual Property in Science (1st ed., pp. 253-279). Routledge.\n\nScientific authorship is a misnomer, a historical vestige. Mario Biagioli, p. 274\n\nThe problems derived from using authorship as an attribution of credit in times of Big Science are a common issue for those studying the social dynamics of science through the use of publications. Authorship and author order are extremely polluted by seniority and power dynamics, furthermore, authorship seems a limited concept for reflecting each author’s contribution and responsibility, especially when dealing with papers produced by large teams.\nScientific authorship is considered a symbolic reward not attached to the specific object (the article) but to the true claims made in such object, and it is given by peers. This means that “a scientific claim does not count as such unless it is made public and subjected to peer evaluation” (p. 254). Multiauthorship includes a layer of complexity to the evaluation of scientific performance, as it introduces ambiguity: evaluators have to assess the value of the paper and the share of such value attributed to the candidate. “[T]he economy of science is inherently based on trust” (p. 260). The author argues that copyright does not hold for our understanding of scientific author and hence proposes a redefinition.\nHe revises how two different fields approached to this issue. The first one is the biomedical sciences. IMCJE’s first solution to this issue, was to reinforce the figure of the author in the traditional sense, which does not work in highly stratified science formed by dozens or even hundreds of authors. Hence it moved towards the concept of contributorship, where team members indicate their specific contribution to a given output, with the order of contributors reflecting the importance of such contribution. One of those contributors would play the role of the guarantor, that is, the person(s) who is responsible for overseeing the whole study and coordinating.\nThe second approach comes from high energy physics, instead of approaching authorship as a matter of responsibility and ownership, it considers it a matter of labour recognition. There is a Standard Author List for all people who have contributed in any way to a given lab or research centre, regardless of their involvement on specific papers or projects, and they appear alphabetically. To be part of this list, researchers have to compromise a share of their time and work, and this list is updated biannually by a committee. Furthermore, leaves of absence are allowed of up to a year without meaning being removed from the list, and researchers are even recognized a year after being removed from the list, as a way to recognize that contributions can be direct or indirect and are cumulative. In this second setting, authorship does not have the same value or credit as in biomedical sciences, and prestige and recognition operate differently through recommendation letters and internal dynamics and correspondence.\nThe review process in high energy physics operates in a different way to what we know from other fields. After a subgroup from the Standard Author List writes up a manuscript, it is submitted internally to all members of the list, who are asked to comment electronically. The paper goes three rounds of internal reviews and then, members of the list who still disagree with the contents, remove their name from the manuscript, hence here less authors would equate to less acceptance. As all the validation process is internal, so are issues with research integrity, fraud or misconduct.\nHence, Biagioli concludes indicating that scientific authorship is a tied more to disciplinary ecologies and the economy of science rather than to a legal category, more related with responsibility and disciplinary norms."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Hello world",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/27-11-2024/index.html",
    "href": "posts/27-11-2024/index.html",
    "title": "Downloading data from Scopus API",
    "section": "",
    "text": "This is a small tutorial on how to work with the Scopus API using R. Most bibliographic databases allow nowadays downloading bibliographic data through their API in an automated way if you are registered and have an institutional subscription. In many cases, they lack of a good documentation, this is not the case for Elsevier, who have quite a lot of info on how to work with their data."
  },
  {
    "objectID": "posts/27-11-2024/index.html#step-1.-setup-api-access",
    "href": "posts/27-11-2024/index.html#step-1.-setup-api-access",
    "title": "Downloading data from Scopus API",
    "section": "Step 1. Setup API Access",
    "text": "Step 1. Setup API Access\nFirst thing you need to do is to go to the Elsevier Developers Portal and request an API Key.\n\nWhen you request a key, it asks you to access through your institutional account and will register an API key under your profile."
  },
  {
    "objectID": "posts/27-11-2024/index.html#step-2.-environment-setup",
    "href": "posts/27-11-2024/index.html#step-2.-environment-setup",
    "title": "Downloading data from Scopus API",
    "section": "Step 2. Environment setup",
    "text": "Step 2. Environment setup\nNow it is time to make sure we have all the packages needed to download and process data. These are:\n\nhttr. Provides functions for working with HTTP requests and responses, making it easy to interact with web APIs\njsonlite. A package to parse, process and generate JSON data.\ntidyverse. A collection of R packages for data manipulation, visualization and analysis. Helps to organize and work with data retrieved from the API.\n\n\n# load libraries\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks jsonlite::flatten()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nNext step is to store the API key in an .Renviron file. For this we first need to open the .Renviron file in RStudio:\n\nfile.edit(\"~/.Renviron\")\n\nAdd a new line to the file with this info:\n\nSCOPUS_API_KEY=your_api_key_here\n\nNow let’s try it works:\n\n# Retrieve API from .Renviron\napi_key &lt;- Sys.getenv(\"SCOPUS_API_KEY\")\n\n# Test API call to check validity of the key\nresponse &lt;- GET(\"https://api.elsevier.com/content/search/scopus\",\n                add_headers(\"X-ELS-APIKey\" = api_key),\n                query = list(query = \"AUTHOR-NAME(Robinson-Garcia)\", count = 1))\n\n# Check status\nif (status_code(response) == 200) {\n  print(\"API key is valid and working.\")\n} else {\n  print(paste(\"Error:\", status_code(response), \"Check your API key or access permissions.\"))\n}\n\n[1] \"API key is valid and working.\""
  },
  {
    "objectID": "posts/27-11-2024/index.html#step-3.-reading-and-preparing-the-list-of-author-ids",
    "href": "posts/27-11-2024/index.html#step-3.-reading-and-preparing-the-list-of-author-ids",
    "title": "Downloading data from Scopus API",
    "section": "Step 3. Reading and preparing the list of Author IDs",
    "text": "Step 3. Reading and preparing the list of Author IDs\nIn my case I already have a list of publications with their author IDs per row. I want to work only with the Author IDs and clean it so that I have one by row in a vector for querying later on the API.\n\nlibrary(readr)     # For reading the CSV file\n\n# Step 1: Import the CSV file\n# Replace \"your_file.csv\" with your actual file path\ndata &lt;- read_csv(\"G:/Mi unidad/1. Work sync/Projects/z2025_01-SELECT/Contributions-inequalites/raw_data/contrib_data.csv\")\n\nRows: 675080 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (7): doi, auid_list, affiliation_id_list, author_affiliation_mapping, s...\ndbl (22): Eid, Year, n_authors, source_id, CitationCount, DDR, DDA, SV_topic...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Step 2: Extract and clean the 'auid_list' column\nauthor_ids &lt;- data %&gt;%\n  select(auid_list) %&gt;%               # Select the relevant column\n  separate_rows(auid_list, sep = \",\") %&gt;% # Split each row by commas\n  mutate(auid_list = str_trim(auid_list)) %&gt;% # Trim whitespace\n  distinct(auid_list) %&gt;%                 # Remove duplicate IDs\n  pull(auid_list)                         # Extract as a vector\n\n# Optional: Check the length of the vector\nlength(author_ids)\n\n[1] 2082499\n\n\nI end up with over 2M authors."
  },
  {
    "objectID": "posts/27-11-2024/index.html#step-4.-query-the-api-for-metadata",
    "href": "posts/27-11-2024/index.html#step-4.-query-the-api-for-metadata",
    "title": "Downloading data from Scopus API",
    "section": "Step 4. Query the API for metadata",
    "text": "Step 4. Query the API for metadata\nLet’s create a function to download the data we want:\n\n# Function to query Scopus API for author metadata\nquery_author &lt;- function(author_id, api_key, output_dir = \"author_data\") {\n  # Ensure the output directory exists\n  if (!dir.exists(output_dir)) dir.create(output_dir)\n  \n  # Construct the API URL\n  url &lt;- paste0(\"https://api.elsevier.com/content/author/author_id/\", author_id)\n  \n  # Query the API\n  response &lt;- GET(url,\n                  add_headers(\"X-ELS-APIKey\" = api_key),\n                  query = list(httpAccept = \"application/json\"))\n  \n  if (status_code(response) == 200) {\n    # Parse the response content\n    content_raw &lt;- content(response, as = \"text\", encoding = \"UTF-8\")\n    content &lt;- fromJSON(content_raw)\n    \n    # Save to a JSON file\n    output_file &lt;- file.path(output_dir, paste0(author_id, \".json\"))\n    write_json(content, output_file, pretty = TRUE)\n    return(TRUE)  # Indicate success\n  } else {\n    # Log the error\n    print(paste(\"Error: Status code\", status_code(response), \"for author ID:\", author_id))\n    return(FALSE)  # Indicate failure\n  }\n}\n\nAnd now a test to see if everything works:\n\nau_data &lt;- query_author(\"36712349900\", api_key)\nprint(au_data)\n\n[1] TRUE"
  },
  {
    "objectID": "posts/27-11-2024/index.html#step-4.-create-loop-and-download",
    "href": "posts/27-11-2024/index.html#step-4.-create-loop-and-download",
    "title": "Downloading data from Scopus API",
    "section": "Step 4. Create loop and download",
    "text": "Step 4. Create loop and download\n\n1. Create the Loop and Batch Logic\n\nImplement a loop to process author IDs in batches.\nWrite a function to distribute batches across API keys for parallel processing.\n\nSteps:\n\nSplit the full list of author IDs into manageable batches.\nAssign batches to available API keys, ensuring even distribution.\nProcess each batch sequentially or in parallel using the respective API key.\nSave the results incrementally as JSON files.\n\n\n\n2. Test the Code with a Small Dataset\n\nUse a sample dataset of 100 author IDs to validate the process before scaling up.\nUse 2 API keys for this test to confirm:\n\nBatch splitting and key assignment work correctly.\nParallelization works as expected.\nJSON files are saved correctly.\n\nTest and document the maximum batch size (e.g., 100, 500, 1000 IDs per batch) that can run smoothly without exceeding memory or rate limits.\n\n\n\n3. Execute the Parallel Download with 50 API Keys\n\nAfter validating the test, set up the full process to use 50 API keys for the entire dataset.\nEnsure:\n\nAPI keys are evenly distributed across batches.\nPauses are implemented between batches if necessary to comply with API rate limits.\nAll data is saved incrementally as JSON files.\n\n\n\n\nAdditional Notes\n\nAPI Key Management:\n\nConfirm that all 50 API keys are valid and have sufficient quotas (20,000 requests/week per key).\nMonitor usage during the process to avoid exceeding limits.\n\nError Handling:\n\nEnsure failed queries are logged (e.g., into an errors.csv file) for retries later.\n\nResuming Progress:\n\nInclude a mechanism to skip already processed IDs by checking for existing JSON files in the output directory.\n\n\n\n\nNext Steps Summary\n\nBatch Testing:\n\nTest batch size limits with a small dataset of 100 IDs using 2 API keys.\n\nParallel Processing:\n\nImplement parallel processing with 50 API keys and scale up for the full dataset.\n\nError Handling & Logs:\n\nLog any failed queries for retries later.\n\nData Storage:\n\nSave each author’s data as a JSON file incrementally."
  }
]